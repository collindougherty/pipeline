# Omit these columns from the dataframe
df <- df[, !(names(df) %in% columns_to_omit)]
df$SLNB <- as.factor(df$SLNB)
library(Matrix)
# Assuming 'df' is your dataframe
# Select categorical columns
categorical_columns <- names(df)[sapply(df, is.factor) | sapply(df, is.character)]
numeric_columns <- names(df)[sapply(df, is.numeric)]
# Start with an empty sparse matrix
combined_sparse_matrix <- NULL
for (column in categorical_columns) {
# Add "Missing" as a level for factor variables
if(is.factor(df[[column]])) {
levels(df[[column]]) <- c(levels(df[[column]]), "Missing")
}
# Replace NA with "Missing"
df[[column]][is.na(df[[column]])] <- "Missing"
# Check if the number of levels (including 'Missing') is 20 or less
num_levels <- length(levels(factor(df[[column]])))
if (num_levels > 1 && num_levels <= 20) {
# Create dummy variables using model.matrix
dummies <- model.matrix(~ . - 1, data = df[column])
# Convert to sparse matrix
sparse_matrix <- as(Matrix(dummies, sparse = TRUE), "CsparseMatrix")
# Combine with the existing combined matrix
if (is.null(combined_sparse_matrix)) {
combined_sparse_matrix <- sparse_matrix
} else {
combined_sparse_matrix <- cbind(combined_sparse_matrix, sparse_matrix)
}
}
}
# Normalize or scale numeric columns
df_scaled <- as.data.frame(lapply(df[numeric_columns], scale))
# Convert scaled numeric data to a sparse matrix
numeric_sparse_matrix <- as(Matrix(as.matrix(df_scaled), sparse = TRUE), "CsparseMatrix")
# Combine numeric and categorical sparse matrices
if (is.null(combined_sparse_matrix)) {
final_matrix <- numeric_sparse_matrix
} else {
final_matrix <- cbind(combined_sparse_matrix, numeric_sparse_matrix)
}
# Final combined matrix for training
#final_matrix <- as(combined_matrix, "CsparseMatrix")
library(xgboost)
# Step 1: Specify the target variable
target_name <- target_name  # Replace with the actual name of your target variable
# Step 2: Extract labels and convert to numeric if it's a factor
labels <- as.numeric(as.factor(df[[target_name]])) - 1
# Step 3: Get levels of the target variable
target_levels <- levels(factor(df[[target_name]]))
# Check if target variable needs one-hot encoding
if (all(df[[target_name]] %in% c(0, 1))) {
# If the target variable is not just 0s and 1s, proceed with one-hot encoding
# Step 4: Determine the names of the one-hot encoded columns in CSR
# This assumes that the one-hot encoding creates names like 'SEXM' and 'SEXF'
# Get levels including 'Missing' if there were NAs initially
target_levels <- levels(factor(df[[target_name]], exclude = NULL))
encoded_col_names <- paste0(target_name, target_levels)
# Step 5: Identify columns to omit from the dimnames of the final_matrix
column_names <- colnames(final_matrix)
columns_to_omit <- match(encoded_col_names, column_names)
# Step 6: Drop columns from feature_matrix
# Filter out any NA values that may have been returned by match in case of non-matches
columns_to_omit <- columns_to_omit[!is.na(columns_to_omit)]
# Remove the columns from the feature_matrix
if (length(columns_to_omit) > 0) {
feature_matrix <- final_matrix[, -columns_to_omit, drop = FALSE]
} else {
# If no columns to omit, use the original matrix
feature_matrix <- final_matrix
warning(paste("Target variable column", target_name, "not found in the CSR matrix."))
}
} else {
# If target variable is already binary, use the matrix as is
feature_matrix <- final_matrix
}
# if (!is.null(columns_to_omit)) {
#   feature_matrix <- final_matrix[, -columns_to_omit]
# } else {
#   feature_matrix <- final_matrix
#   warning(paste("Target variable column", target_name, "not found in the CSR matrix."))
# }
# Now, you can proceed with splitting the feature_matrix into training and testing sets
# and use it to train your XGBoost model.
# Split data into training and testing sets
set.seed(123)  # for reproducibility
train_indices <- sample(seq_len(nrow(feature_matrix)), size = floor(0.8 * nrow(feature_matrix)))
test_indices <- setdiff(seq_len(nrow(feature_matrix)), train_indices)
train_features <- feature_matrix[train_indices, ]
test_features <- feature_matrix[test_indices, ]
train_labels <- labels[train_indices]
test_labels <- labels[test_indices]
# Define parameters
params <- list(booster = "gbtree", objective = "binary:logistic")
# Train the model
xgb_model <- xgboost(params = params, data = train_features, label = train_labels, nrounds = 100)
# Make predictions
predictions <- predict(xgb_model, test_features)
predicted_labels <- ifelse(predictions > 0.5, 1, 0)
# Evaluate model performance
accuracy <- sum(predicted_labels == test_labels) / length(test_labels)
print(paste("Accuracy:", accuracy))
# Load necessary library for plotting
library(ggplot2)
# Plot feature importance
importance_matrix <- xgb.importance(model = xgb_model)
xgb.plot.importance(importance_matrix)
# Check and install 'caret' package if not already installed
if (!require(caret, quietly = TRUE)) {
install.packages("caret")
library(caret)
}
# Check and install 'e1071' package if not already installed
if (!require(e1071, quietly = TRUE)) {
install.packages("e1071")
library(e1071)
}
# Assuming predicted_labels and test_labels are already defined from the xgboost model
# Make predictions
predictions <- predict(xgb_model, test_features)
predicted_labels <- ifelse(predictions > 0.5, 1, 0)
# Create a confusion matrix
conf_matrix <- confusionMatrix(as.factor(predicted_labels), as.factor(test_labels))
# Print the confusion matrix
print(conf_matrix$table)
# Print overall accuracy from the confusion matrix
print(paste("Accuracy:", conf_matrix$overall['Accuracy']))
# Calculate and print the Kappa statistic
kappa_stat <- conf_matrix$overall['Kappa']
print(paste("Kappa Statistic:", kappa_stat))
View(importance_matrix)
library(keras)
install.packages('keras')
library(keras)
# Assuming 'feature_matrix' and 'labels' are prepared and ready for training
# Splitting data into training and testing sets
set.seed(123)
indices <- sample(1:nrow(feature_matrix), size = floor(0.8 * nrow(feature_matrix)))
x_train <- feature_matrix[indices, ]
y_train <- labels[indices]
x_test <- feature_matrix[-indices, ]
y_test <- labels[-indices]
# Define the model
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = 'relu', input_shape = ncol(feature_matrix)) %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_dense(units = 1, activation = 'sigmoid')
# Load necessary libraries
library(reticulate)
library(keras)
# Install TensorFlow
install_tensorflow()
# Load necessary libraries
library(reticulate)
library(keras)
# Install TensorFlow
tensorflow::install_tensorflow()
# Assuming 'feature_matrix' and 'labels' are prepared and ready for training
# Splitting data into training and testing sets
set.seed(123)
indices <- sample(1:nrow(feature_matrix), size = floor(0.8 * nrow(feature_matrix)))
x_train <- feature_matrix[indices, ]
df <- read.csv("/Users/collindougherty/Documents/Work/pipeline/tests/test_full_recode_breast.csv")
dtype <- function(df) {
# if variable type chr, then change to factor for all vars in df
df <- df %>% mutate_if(is.character, as.factor)
# lets remove all dtype logi columns from df (these are most likely columns not in data dictionary)
df <- df %>% select_if(function(x) !is.logical(x))
return(df)
}
library(tidyverse)
data <- dtype(df)
#df <- head(data,1000)
df <- data
# Specify the target variable
target_name <- "SLNB"  # Replace with your target variable name
# Remove rows with NA in the target variable
df <- df[!is.na(df[[target_name]]), ]
# Columns to be omitted
columns_to_omit <- c("SLN_EXAM", "SLN_POS", "SLN_EXAM_notes", "SLN_POS_notes", "SENTINEL_LNBX_STARTED_DAY", "REGIONAL_NODES_EXAMINED", "REGIONAL_NODES_POSITIVE", "RX_SUMM_SCOPE_REG_LN_2012", "REG_LN_DISS_STARTED_DAY",
"RX_SUMM_SCOPE_REG_LN_SUR", "AJCC_TNM_CLIN_N", "AJCC_TNM_PATH_N_SFX", "AJCC_TNM_PATH_STG_GRP")
# Omit these columns from the dataframe
df <- df[, !(names(df) %in% columns_to_omit)]
df$SLNB <- as.factor(df$SLNB)
library(Matrix)
# Assuming 'df' is your dataframe
# Select categorical columns
categorical_columns <- names(df)[sapply(df, is.factor) | sapply(df, is.character)]
numeric_columns <- names(df)[sapply(df, is.numeric)]
# Start with an empty sparse matrix
combined_sparse_matrix <- NULL
for (column in categorical_columns) {
# Add "Missing" as a level for factor variables
if(is.factor(df[[column]])) {
levels(df[[column]]) <- c(levels(df[[column]]), "Missing")
}
# Replace NA with "Missing"
df[[column]][is.na(df[[column]])] <- "Missing"
# Check if the number of levels (including 'Missing') is 20 or less
num_levels <- length(levels(factor(df[[column]])))
if (num_levels > 1 && num_levels <= 20) {
# Create dummy variables using model.matrix
dummies <- model.matrix(~ . - 1, data = df[column])
# Convert to sparse matrix
sparse_matrix <- as(Matrix(dummies, sparse = TRUE), "CsparseMatrix")
# Combine with the existing combined matrix
if (is.null(combined_sparse_matrix)) {
combined_sparse_matrix <- sparse_matrix
} else {
combined_sparse_matrix <- cbind(combined_sparse_matrix, sparse_matrix)
}
}
}
# Normalize or scale numeric columns
df_scaled <- as.data.frame(lapply(df[numeric_columns], scale))
# Convert scaled numeric data to a sparse matrix
numeric_sparse_matrix <- as(Matrix(as.matrix(df_scaled), sparse = TRUE), "CsparseMatrix")
# Combine numeric and categorical sparse matrices
if (is.null(combined_sparse_matrix)) {
final_matrix <- numeric_sparse_matrix
} else {
final_matrix <- cbind(combined_sparse_matrix, numeric_sparse_matrix)
}
# Final combined matrix for training
#final_matrix <- as(combined_matrix, "CsparseMatrix")
library(xgboost)
# Step 1: Specify the target variable
target_name <- target_name  # Replace with the actual name of your target variable
# Step 2: Extract labels and convert to numeric if it's a factor
labels <- as.numeric(as.factor(df[[target_name]])) - 1
# Step 3: Get levels of the target variable
target_levels <- levels(factor(df[[target_name]]))
# Check if target variable needs one-hot encoding
if (all(df[[target_name]] %in% c(0, 1))) {
# If the target variable is not just 0s and 1s, proceed with one-hot encoding
# Step 4: Determine the names of the one-hot encoded columns in CSR
# This assumes that the one-hot encoding creates names like 'SEXM' and 'SEXF'
# Get levels including 'Missing' if there were NAs initially
target_levels <- levels(factor(df[[target_name]], exclude = NULL))
encoded_col_names <- paste0(target_name, target_levels)
# Step 5: Identify columns to omit from the dimnames of the final_matrix
column_names <- colnames(final_matrix)
columns_to_omit <- match(encoded_col_names, column_names)
# Step 6: Drop columns from feature_matrix
# Filter out any NA values that may have been returned by match in case of non-matches
columns_to_omit <- columns_to_omit[!is.na(columns_to_omit)]
# Remove the columns from the feature_matrix
if (length(columns_to_omit) > 0) {
feature_matrix <- final_matrix[, -columns_to_omit, drop = FALSE]
} else {
# If no columns to omit, use the original matrix
feature_matrix <- final_matrix
warning(paste("Target variable column", target_name, "not found in the CSR matrix."))
}
} else {
# If target variable is already binary, use the matrix as is
feature_matrix <- final_matrix
}
# if (!is.null(columns_to_omit)) {
#   feature_matrix <- final_matrix[, -columns_to_omit]
# } else {
#   feature_matrix <- final_matrix
#   warning(paste("Target variable column", target_name, "not found in the CSR matrix."))
# }
# Now, you can proceed with splitting the feature_matrix into training and testing sets
# and use it to train your XGBoost model.
# Split data into training and testing sets
set.seed(123)  # for reproducibility
train_indices <- sample(seq_len(nrow(feature_matrix)), size = floor(0.8 * nrow(feature_matrix)))
test_indices <- setdiff(seq_len(nrow(feature_matrix)), train_indices)
train_features <- feature_matrix[train_indices, ]
test_features <- feature_matrix[test_indices, ]
train_labels <- labels[train_indices]
test_labels <- labels[test_indices]
# Define parameters
params <- list(booster = "gbtree", objective = "binary:logistic")
# Train the model
xgb_model <- xgboost(params = params, data = train_features, label = train_labels, nrounds = 100)
# Make predictions
predictions <- predict(xgb_model, test_features)
predicted_labels <- ifelse(predictions > 0.5, 1, 0)
# Evaluate model performance
accuracy <- sum(predicted_labels == test_labels) / length(test_labels)
print(paste("Accuracy:", accuracy))
# Load necessary libraries
library(reticulate)
library(keras)
# Install TensorFlow
# tensorflow::install_tensorflow()
# Assuming 'feature_matrix' and 'labels' are prepared and ready for training
# Splitting data into training and testing sets
set.seed(123)
indices <- sample(1:nrow(feature_matrix), size = floor(0.8 * nrow(feature_matrix)))
x_train <- feature_matrix[indices, ]
y_train <- labels[indices]
x_test <- feature_matrix[-indices, ]
y_test <- labels[-indices]
# Define the model
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = 'relu', input_shape = ncol(feature_matrix)) %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_dense(units = 1, activation = 'sigmoid')
# Function to extract variable names
extract_variable_name <- function(line) {
matches <- regmatches(line, regexec("df\\$(.*?)\\s*<-", line))
if (length(matches[[1]]) > 1) {
return(matches[[1]][2])  # The variable name is in the second capture group
} else {
return(NA)  # Return NA if no match found
}
}
# Function to extract levels
extract_levels <- function(lines) {
pattern <- "levels\\s*=\\s*c\\((.*?)\\)"
for (line in lines) {
if (grepl(pattern, line)) {
matches <- regmatches(line, regexec(pattern, line))
return(matches[[1]][2])  # The levels are in the second capture group
}
}
return(NA)  # Return NA if no match found
}
# Function to extract labels
extract_labels <- function(lines) {
labels <- ""
capturing <- FALSE
for (line in lines) {
if (grepl("labels\\s*=\\s*c\\(", line)) {
capturing <- TRUE  # Start capturing lines
}
if (capturing) {
labels <- paste0(labels, line)
}
if (capturing && grepl("\\)$", line)) {
break  # Stop if end of definition ")"
}
}
if (capturing) {
matches <- regmatches(labels, regexec("c\\((.*)\\)", labels))
if (length(matches[[1]]) > 1) {
return(matches[[1]][2])  # The labels are in the second capture group
}
}
return(NA)  # Return NA if no match found
}
# Initialize an empty data frame for logging factor variables
factor_log <- data.frame(VariableName = character(),
Levels = character(),
Labels = character(),
stringsAsFactors = FALSE)
# Read the R script
script_lines <- readLines("/Users/collindougherty/Documents/Work/pipeline/backend/ncdb_recode.R")  # Make sure to provide the correct path
# Parse the script for factor variable definitions
for (i in 1:(length(script_lines))) {
line <- script_lines[i]
if (grepl("df\\$.*<-\\s*factor\\(", line)) {
var_name <- extract_variable_name(line)
# Assume next lines contain levels and labels
levels <- extract_levels(script_lines[i:(i+10)])
labels <- extract_labels(script_lines[i:(i+20)])
# Log in DataFrame
factor_log <- rbind(factor_log, data.frame(VariableName = var_name,
Levels = levels,
Labels = labels))
}
}
# Write to CSV
write.csv(factor_log, "factor_variables_log.csv", row.names = FALSE)
# Print a confirmation message
print("Factor variables logged and written to factor_variables_log.csv")
# Function to extract variable names
extract_variable_name <- function(line) {
matches <- regmatches(line, regexec("df\\$(.*?)\\s*<-", line))
if (length(matches[[1]]) > 1) {
return(matches[[1]][2])  # The variable name is in the second capture group
} else {
return(NA)  # Return NA if no match found
}
}
# Function to extract levels
extract_levels <- function(lines) {
pattern <- "levels\\s*=\\s*c\\((.*?)\\)"
for (line in lines) {
if (grepl(pattern, line)) {
matches <- regmatches(line, regexec(pattern, line))
return(matches[[1]][2])  # The levels are in the second capture group
}
}
return(NA)  # Return NA if no match found
}
# Function to extract labels
extract_labels <- function(lines) {
labels <- ""
capturing <- FALSE
for (line in lines) {
if (grepl("labels\\s*=\\s*c\\(", line)) {
capturing <- TRUE  # Start capturing lines
}
if (capturing) {
labels <- paste0(labels, line)
}
if (capturing && grepl("\\)$", line)) {
break  # Stop if end of definition ")"
}
}
if (capturing) {
matches <- regmatches(labels, regexec("c\\((.*)\\)", labels))
if (length(matches[[1]]) > 1) {
return(matches[[1]][2])  # The labels are in the second capture group
}
}
return(NA)  # Return NA if no match found
}
# Initialize an empty data frame for logging factor variables
factor_log <- data.frame(VariableName = character(),
Levels = character(),
Labels = character(),
stringsAsFactors = FALSE)
# Read the R script
script_path <- "/Users/collindougherty/Documents/Work/pipeline/backend/ncdb_recode.R"
script_lines <- readLines(script_path)  # Make sure to provide the correct path
# Parse the script for factor variable definitions
for (i in 1:(length(script_lines))) {
line <- script_lines[i]
if (grepl("df\\$.*<-\\s*factor\\(", line)) {
var_name <- extract_variable_name(line)
# Assume next lines contain levels and labels
levels <- extract_levels(script_lines[i:(i+10)])
labels <- extract_labels(script_lines[i:(i+20)])
# Log in DataFrame
factor_log <- rbind(factor_log, data.frame(VariableName = var_name,
Levels = levels,
Labels = labels))
}
}
# Write to CSV
write.csv(factor_log, "factor_variables_log.csv", row.names = FALSE)
# Print a confirmation message
print("Factor variables logged and written to factor_variables_log.csv")
rm(list=ls())
# Function to extract variable names
extract_variable_name <- function(line) {
matches <- regmatches(line, regexec("df\\$(.*?)\\s*<-", line))
if (length(matches[[1]]) > 1) {
return(matches[[1]][2])  # The variable name is in the second capture group
} else {
return(NA)  # Return NA if no match found
}
}
# Function to extract levels
extract_levels <- function(lines) {
pattern <- "levels\\s*=\\s*c\\((.*?)\\)"
for (line in lines) {
if (grepl(pattern, line)) {
matches <- regmatches(line, regexec(pattern, line))
return(matches[[1]][2])  # The levels are in the second capture group
}
}
return(NA)  # Return NA if no match found
}
# Function to extract labels
extract_labels <- function(lines) {
labels <- ""
capturing <- FALSE
for (line in lines) {
if (grepl("labels\\s*=\\s*c\\(", line)) {
capturing <- TRUE  # Start capturing lines
}
if (capturing) {
labels <- paste0(labels, line)
}
if (capturing && grepl("\\)$", line)) {
break  # Stop if end of definition ")"
}
}
if (capturing) {
matches <- regmatches(labels, regexec("c\\((.*)\\)", labels))
if (length(matches[[1]]) > 1) {
return(matches[[1]][2])  # The labels are in the second capture group
}
}
return(NA)  # Return NA if no match found
}
# Initialize an empty data frame for logging factor variables
factor_log <- data.frame(VariableName = character(),
Levels = character(),
Labels = character(),
stringsAsFactors = FALSE)
# Read the R script
script_path <- "/Users/collindougherty/Documents/Work/pipeline/backend/ncdb_recode.R"
script_lines <- readLines(script_path)  # Make sure to provide the correct path
# Parse the script for factor variable definitions
for (i in 1:(length(script_lines))) {
line <- script_lines[i]
if (grepl("df\\$.*<-\\s*factor\\(", line)) {
var_name <- extract_variable_name(line)
# Assume next lines contain levels and labels
levels <- extract_levels(script_lines[i:(i+10)])
labels <- extract_labels(script_lines[i:(i+20)])
# Log in DataFrame
factor_log <- rbind(factor_log, data.frame(VariableName = var_name,
Levels = levels,
Labels = labels))
}
}
# Write to CSV
write.csv(factor_log, "factor_variables_log.csv", row.names = FALSE)
# Print a confirmation message
print("Factor variables logged and written to factor_variables_log.csv")
View(factor_log)
View(factor_log)
